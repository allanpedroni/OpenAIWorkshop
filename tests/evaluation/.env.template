# Agent Evaluation Environment Configuration
# Copy this file to .env and fill in your Azure OpenAI credentials

# ═══════════════════════════════════════════════════════════════════════════════
# AZURE OPENAI CONFIGURATION (Required)
# ═══════════════════════════════════════════════════════════════════════════════
AZURE_OPENAI_ENDPOINT="https://your-endpoint.openai.azure.com"
AZURE_OPENAI_KEY="your-api-key-here"
AZURE_OPENAI_API_KEY="your-api-key-here"  # Alias for compatibility
AZURE_OPENAI_CHAT_DEPLOYMENT="gpt-4.1"
AZURE_OPENAI_DEPLOYMENT="gpt-4.1"  # For LLM-as-judge evaluators
AZURE_OPENAI_API_VERSION="2025-03-01-preview"

# ═══════════════════════════════════════════════════════════════════════════════
# LLM-AS-JUDGE CONFIGURATION (For AI-assisted evaluation)
# ═══════════════════════════════════════════════════════════════════════════════
# Model deployment for LLM judge evaluators (recommend gpt-4o or better)
# Set USE_REASONING_MODEL=true if using o-series models (o1, o3-mini)
LLM_JUDGE_DEPLOYMENT="gpt-4o"
USE_REASONING_MODEL="false"

# ═══════════════════════════════════════════════════════════════════════════════
# MCP SERVER CONFIGURATION (Required for integration tests)
# ═══════════════════════════════════════════════════════════════════════════════
MCP_SERVER_URI="http://localhost:8000/mcp"

# ═══════════════════════════════════════════════════════════════════════════════
# AGENT MODULES TO EVALUATE
# ═══════════════════════════════════════════════════════════════════════════════
# Single Agent - Basic intelligent agent with MCP tools
SINGLE_AGENT_MODULE="agents.agent_framework.single_agent"

# Reflection Agent - Primary + Reviewer pattern for quality assurance
REFLECTION_AGENT_MODULE="agents.agent_framework.multi_agent.reflection_agent"

# Default agent for single-agent tests
DEFAULT_AGENT_MODULE="agents.agent_framework.single_agent"

# ═══════════════════════════════════════════════════════════════════════════════
# EVALUATION SETTINGS
# ═══════════════════════════════════════════════════════════════════════════════
# Number of test cases to run in quick mode (default: 3)
EVAL_QUICK_TEST_COUNT=3

# Enable LLM-as-judge evaluation (uses additional Azure OpenAI calls)
# When true, uses Azure AI Foundry evaluators (IntentResolution, TaskAdherence, etc.)
# When false, uses simple keyword matching for outcome evaluation
EVAL_USE_LLM_JUDGE="true"

# Tool call accuracy threshold (0.0 - 1.0)
EVAL_TOOL_ACCURACY_THRESHOLD=0.5

# ═══════════════════════════════════════════════════════════════════════════════
# OPTIONAL: Embedding model for AI evaluation
# ═══════════════════════════════════════════════════════════════════════════════
AZURE_OPENAI_EMBEDDING_DEPLOYMENT="text-embedding-ada-002"
