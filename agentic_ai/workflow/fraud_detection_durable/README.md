# Durable Agentic Workflows â€” A Design Pattern for Long-Running AI Agent Orchestration

## The Problem: AI Agents That Must Not Forget

Most AI agent demos run in a single request-response cycle: user asks, agent thinks, agent answers. But real-world agent systems need to do things that take **minutes, hours, or days** â€” and they must survive failures along the way.

Consider what happens when an AI agent needs to:

- **Monitor transactions 24/7 for fraud** â€” continuously ingest telemetry events, detect anomalies in real time, autonomously launch multi-agent investigations, then wait for a human analyst to approve or reject the recommended action (which might take hours or happen over a weekend). This is an *ambient agent* â€” it runs in the background with no user prompt, watching for patterns that require intervention
- **Manage an IT incident** â€” detect anomaly, triage severity, page on-call engineer, wait for acknowledgment (with escalation if no response in 15 minutes), coordinate remediation steps, produce post-mortem
- **Process a loan application** â€” pull credit reports, verify employment, run compliance checks in parallel, wait for underwriter approval, then execute the disbursement
- **Orchestrate a supply chain order** â€” validate inventory across warehouses, reserve stock, calculate shipping, wait for supplier confirmation, handle partial fulfillments, retry failed shipments

These scenarios share a common trait: **no human initiates the work**. Events arrive continuously â€” transactions, alerts, sensor readings, log entries â€” and the agent must watch, decide, act, and sometimes wait for human input before proceeding. This is fundamentally different from chatbot-style "user asks, agent answers" interactions.

These are **long-running, multi-step, stateful workflows** where:

1. **Human decisions inject unbounded delays** â€” an analyst might respond in 5 minutes or 5 days
2. **Multiple agents collaborate** â€” a router dispatches to specialists, an aggregator synthesizes results
3. **Failures are inevitable** â€” processes crash, containers restart, VMs get preempted, network connections drop
4. **Actions must not be repeated** â€” you can't lock a bank account twice or charge a customer twice because the worker restarted

The fundamental question is: **when the process running your agent dies, what happens to all the work it already completed?**

---

## Why Building This Yourself Is Harder Than You Think

The instinct is to reach for familiar tools: "I'll just checkpoint my state to a database or blob storage." This works for simple linear flows, but falls apart in real agent orchestration scenarios. Here are the five problems you'll need to solve â€” and each one is a distributed systems project in its own right.

### 1. The Wait-for-Human Problem

Your orchestration reaches a point where it needs a human decision. The agent has produced a risk assessment; now an analyst must approve or reject.

**DIY approach:** Save current state to blob â†’ poll a database for the decision â†’ resume.

**What goes wrong:**
- You need an event subscription system: when the analyst submits a decision via your API, something must correlate that decision back to the specific waiting orchestration instance
- You need a timeout mechanism: if no decision arrives in 72 hours, escalate automatically
- You need both to race: whichever fires first (human decision or timeout) wins, and the other must be cancelled cleanly
- All of this must survive process restarts â€” the timer can't live in `asyncio`; the event subscription can't live in a Python dict

With a durable orchestrator, this is one line:

```python
winner = yield when_any([
    context.wait_for_external_event("AnalystDecision"),
    context.create_timer(timedelta(hours=72))
])
```

The event subscription, the timer, and the race â€” all persisted in the orchestrator's storage, not in worker memory.

### 2. The Replay/Resume Problem

Your workflow has 8 steps. It crashed after step 5. How do you resume at step 6?

**DIY approach:** Save a `current_step` counter â†’ on restart, load it â†’ use a giant if/elif chain to jump to the right step.

**What goes wrong:**
- Each step's *output* was consumed by later steps. You need to save every intermediate result, not just the step number
- If your workflow has branches (`if risk > 0.6: wait for human; else: auto-clear`), the if/elif chain grows exponentially
- If your workflow has loops (analyst rejects â†’ re-investigate â†’ wait again), the state machine becomes nearly impossible to maintain
- Every time you change the workflow logic, you must update the resume logic in lockstep

With a durable orchestrator, there is no resume logic. The framework **replays your function from the beginning**, returning cached results for completed steps and suspending at the first incomplete step. Your code is the state machine.

### 3. The Concurrency Problem

Multiple worker instances are running (for scale). Two workers pick up the same orchestration after a restart.

**DIY approach:** Distributed locks via Redis or blob leases â†’ acquire before processing â†’ release after.

**What goes wrong:**
- Lock expiration vs. long-running work: if the LLM call takes 30 seconds and your lock expires at 15, another worker enters
- Deadlocks when one worker holds lock A and waits for lock B while another holds lock B and waits for lock A
- Partial completion: worker acquires lock, completes 3 of 5 steps, crashes â€” now you have half-done state and need rollback

A durable orchestrator manages concurrency internally via its event store â€” each orchestration instance has an atomic event stream with built-in sequencing.

### 4. The Exactly-Once Side Effects Problem

Step 5 is "lock the customer's bank account." The worker executed it, but crashed before recording that it succeeded.

**DIY approach:** Idempotency keys for every side effect â†’ check before executing â†’ mark after.

**What goes wrong:**
- You need an idempotency store separate from your state store (they must be updated atomically or you get inconsistencies)
- Every activity function must be wrapped with idempotency-key generation, lookup, and recording
- The pattern is different for every external system (REST API idempotency key vs. database upsert vs. message dedup)

A durable orchestrator treats activities as **events in an append-only log**. If the worker crashes after the activity completes but before it records the result, the replay will see the completion event and skip re-execution automatically.

### 5. The Stateful Agent Conversation Problem

Your agent needs to re-investigate with full context: the original alert, its first analysis, the analyst's feedback. This conversation must persist across process restarts and even across multiple reject-reinvestigate cycles.

**DIY approach:** Serialize the chat history to blob/database â†’ load on each call â†’ append â†’ save.

**What goes wrong:**
- Concurrent writes: two activities might try to update the same conversation simultaneously
- You need optimistic concurrency (ETags) or pessimistic locking
- Schema evolution: as your agent's message format changes, old serialized conversations must still deserialize
- Garbage collection: conversations that are complete should eventually be pruned

A durable orchestrator with **entity state** (like DTS entities) handles all of this: atomic reads/writes, built-in concurrency control, and structured state that the framework manages.

---

### The Honest Assessment

| Concern | DIY Blob/DB Checkpointing | Durable Orchestrator (DTS) |
|---------|---------------------------|---------------------------|
| Simple linear pipeline, no human wait | âœ… Works fine, less infra | âš ï¸ Overkill |
| Human-in-the-loop with timeout | ğŸ”´ Build entire event system | âœ… `when_any([event, timer])` |
| Crash recovery with branches/loops | ğŸ”´ Exponential state machine | âœ… Automatic replay |
| Multi-worker concurrency | ğŸ”´ Distributed locks | âœ… Built-in event sequencing |
| Exactly-once side effects | ğŸ”´ Idempotency infrastructure | âœ… Activity completion log |
| Persistent agent conversations | ğŸŸ¡ Possible but manual | âœ… Entity state with concurrency |

**Bottom line:** If your agent workflow is a straight-line script with no human waits, DIY checkpointing works. The moment you add human-in-the-loop, branching, loops, or multi-step crash recovery, you're building a workflow engine â€” and building a *correct* one is a multi-year distributed systems project.

---

## Enter: Azure Durable Task Scheduler (DTS)

The [Azure Durable Task Scheduler](https://learn.microsoft.com/en-us/azure/durable-task-scheduler/) is a managed service that provides exactly the primitives needed for durable agent orchestration:

```mermaid
%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#4A90D9', 'primaryTextColor': '#fff' }}}%%
flowchart LR
    subgraph DTS_SCOPE["ğŸ”· What DTS Provides"]
        direction TB
        DS1["ğŸ“‹ Persistent task queue"]
        DS2["ğŸ“ Event store â€” append-only log"]
        DS3["â±ï¸ Timer service â€” survives restarts"]
        DS4["ğŸ—ƒï¸ Entity state storage"]
        DS5["ğŸ“¦ Work item distribution"]
        DS6["ğŸ” At-least-once delivery"]
    end

    subgraph WORKER_SCOPE["ğŸ”§ What Your Code Provides"]
        direction TB
        WS1["ğŸ Compute runtime â€” Python"]
        WS2["ğŸ¤– LLM calls â€” Azure OpenAI"]
        WS3["ğŸ”Œ Tool calls â€” MCP, APIs"]
        WS4["ğŸ“Š Business logic â€” orchestration"]
        WS5["ğŸ§© Agent framework integration"]
    end

    DTS_SCOPE ~~~ WORKER_SCOPE

    style DTS_SCOPE fill:#cce5ff,stroke:#004085,stroke-width:2px
    style WORKER_SCOPE fill:#ffeeba,stroke:#ffc107,stroke-width:2px
```

**DTS is a persistent task queue + event store + timer service.** It doesn't run your code â€” it stores the *record of what happened* and *dispatches work items* to workers. Your worker is a **stateless compute runtime** that pulls tasks, executes your Python/LLM/MCP logic, and reports results back. If the worker dies, DTS still has the full event log; a new worker replays it and picks up where things left off.

### Key Properties

| Property | How It Works |
|----------|-------------|
| **Checkpointing** | Every `yield` in your orchestration writes a completion event to DTS's append-only log |
| **Crash recovery** | Worker replays the orchestration function; completed yields return cached results instantly |
| **Human-in-the-loop** | `wait_for_external_event()` creates a subscription in DTS storage â€” survives indefinitely |
| **Timers** | `create_timer(72h)` fires in DTS's timer service â€” not in your process memory |
| **Entity state** | Agent conversation history persisted as a structured entity, with atomic updates |
| **Scaling** | Multiple workers pull from the same task hub â€” DTS distributes work items |

### Development Experience

DTS ships a **local emulator** as a Docker container, so you develop and test locally without an Azure subscription:

```bash
# Local development â€” zero cloud dependency
docker run -d --name dts-emulator -p 8080:8080 mcr.microsoft.com/dts/dts-emulator:latest

# Production â€” same SDK, same code, just change the endpoint
DTS_ENDPOINT=https://your-dts.northcentralus.durabletask.io
```

---

## Reference Architecture: Ambient Fraud Detection

To prove these patterns, we built a complete fraud detection system that exercises every durable orchestration primitive: fan-out/fan-in agents, human-in-the-loop with timeout, stateful feedback loops, and crash recovery.

### The 3-Layer Design

```mermaid
%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#4A90D9', 'primaryTextColor': '#fff', 'lineColor': '#5C6B77' }}}%%
flowchart TB
    subgraph LAYER1["ğŸŸ¢ Layer 1 â€” Detection Â· Ambient Monitoring"]
        direction LR
        TELEMETRY["ğŸ“Š Telemetry<br/>Generator<br/><i>2â€“5s interval</i>"]
        RULES["âš¡ Rule Engine<br/><i>Python, no LLM</i><br/>â€¢ Multi-country login<br/>â€¢ Spending spike > 3Ã—<br/>â€¢ Burst API calls<br/>â€¢ Repeated auth failures"]
        SUBMIT["ğŸš¨ Auto-submit<br/>POST /api/workflow/start"]
        SSE_OUT["ğŸ“º SSE Stream<br/>â†’ React Live Feed"]

        TELEMETRY --> RULES
        RULES -->|"anomaly detected"| SUBMIT
        TELEMETRY -->|"all events"| SSE_OUT
        RULES -->|"flagged events"| SSE_OUT
    end

    subgraph LAYER2["ğŸ”µ Layer 2 â€” Investigation Â· Durable Agent Orchestration"]
        direction TB
        ORCH["ğŸ”· DTS Orchestration<br/><i>checkpointed, crash-recoverable</i>"]

        subgraph ENTITY["dafx-FraudAnalysisAgent Entity"]
            direction TB
            INNER["Inner Workflow<br/><i>(fast, async, in-memory)</i>"]

            subgraph FANOUT["Fan-out / Fan-in"]
                direction LR
                ROUTER["AlertRouter"] --> USAGE["ğŸ“ˆ Usage<br/>Analyst"]
                ROUTER --> LOCATION["ğŸŒ Location<br/>Analyst"]
                ROUTER --> BILLING["ğŸ’³ Billing<br/>Analyst"]
            end

            USAGE -->|"MCP tools"| AGG["ğŸ§  FraudRisk<br/>Aggregator<br/><i>(LLM)</i>"]
            LOCATION -->|"MCP tools"| AGG
            BILLING -->|"MCP tools"| AGG
            AGG --> ASSESSMENT["ğŸ“‹ FraudRisk<br/>Assessment"]

            INNER --- FANOUT
        end

        ORCH -->|"yield fraud_agent.run()"| ENTITY
    end

    subgraph LAYER3["ğŸŸ  Layer 3 â€” Decision & Action Â· HITL + Execution"]
        direction TB
        RISK_CHECK{"Risk â‰¥ 0.6?"}

        subgraph HITL_LOOP["HITL Feedback Loop<br/><i>durable, stateful, crash-safe</i>"]
            direction TB
            NOTIFY["ğŸ“§ Notify Analyst<br/><i>DTS Activity</i>"]
            WAIT["â³ Wait for Decision<br/><i>when_any(event, 72h timer)</i><br/><b>Survives process death</b>"]
            DECIDE{"Analyst<br/>Decision?"}
            EXECUTE["âœ… Execute Action<br/><i>DTS Activity</i>"]
            REINVEST["ğŸ”„ Re-investigate<br/><i>same session = full history</i>"]
            TIMEOUT_ACT["â° Escalate Timeout<br/><i>DTS Activity</i>"]
        end

        AUTO_CLEAR["ğŸŸ¢ Auto-clear<br/><i>DTS Activity</i>"]
        FINAL["ğŸ“¨ Send Notification<br/><i>DTS Activity</i>"]

        RISK_CHECK -->|"YES"| NOTIFY
        NOTIFY --> WAIT
        WAIT --> DECIDE
        DECIDE -->|"Approve"| EXECUTE
        DECIDE -->|"Reject + feedback"| REINVEST
        DECIDE -->|"Timeout"| TIMEOUT_ACT
        REINVEST -->|"loop back"| NOTIFY
        RISK_CHECK -->|"NO"| AUTO_CLEAR

        EXECUTE --> FINAL
        TIMEOUT_ACT --> FINAL
        AUTO_CLEAR --> FINAL
    end

    SUBMIT -->|"triggers"| ORCH
    ENTITY -->|"risk score"| RISK_CHECK

    style LAYER1 fill:#d4edda,stroke:#28a745,stroke-width:2px
    style LAYER2 fill:#cce5ff,stroke:#004085,stroke-width:2px
    style LAYER3 fill:#fff3cd,stroke:#856404,stroke-width:2px
    style ENTITY fill:#e8f4fd,stroke:#4A90D9
    style HITL_LOOP fill:#ffeeba,stroke:#ffc107
    style FANOUT fill:#f0f4f8,stroke:#adb5bd
```

### Why This Layering?

| Layer | Uses LLM? | Durable? | Rationale |
|-------|-----------|----------|-----------|
| **Layer 1** â€” Detection | âŒ No | âŒ No | Events arrive every 2â€“5s. An LLM call takes 2â€“5s and costs money. Simple rules catch 95% of benign events at zero cost. |
| **Layer 2** â€” Investigation | âœ… Yes | âœ… Yes (entity) | Complex multi-signal reasoning is the LLM's strength. Entity state persists the full conversation for re-investigation. |
| **Layer 3** â€” Decision | âŒ No | âœ… Yes (orchestration) | Human decisions can take hours/days. DTS timers and external events survive crashes and restarts. |

### Service Topology

The system runs as four independent processes communicating through DTS and HTTP:

```mermaid
flowchart LR
    BROWSER["ğŸŒ Browser<br/>(REST / SSE / WS)"]
    BACKEND["âš™ï¸ Backend<br/>(FastAPI)"]
    DTS["ğŸ”· DTS<br/>(gRPC SDK)"]
    WORKER["ğŸ”§ Worker"]
    MCP["ğŸ”Œ MCP Server"]

    BROWSER -->|"HTTP"| BACKEND
    BACKEND -->|"gRPC"| DTS
    DTS -.->|"pull"| WORKER
    WORKER -->|"HTTP"| MCP

    style BROWSER fill:#e8f4fd,stroke:#4A90D9
    style BACKEND fill:#d4edda,stroke:#28a745
    style DTS fill:#cce5ff,stroke:#004085
    style WORKER fill:#ffeeba,stroke:#ffc107
    style MCP fill:#d6d8db,stroke:#6c757d
```

The browser cannot call DTS's gRPC SDK directly. The FastAPI backend acts as a Backend-For-Frontend (BFF), translating REST/WebSocket/SSE into SDK calls. In production, swap `DTS_ENDPOINT` from `localhost:8080` to your Azure DTS endpoint â€” **zero code changes**.

---

## Key Design Patterns

### Pattern 1: Durability Boundaries â€” What Gets Checkpointed

Not everything needs to be durable. The key architectural insight is choosing **where** to draw the durability boundary:

```mermaid
%%{init: {'theme': 'base'}}%%
flowchart LR
    subgraph DURABLE["âœ… DTS-Managed Â· each yield = checkpoint"]
        direction TB
        D1["yield fraud_agent.run(...)"]
        D2["yield wait_for_external_event()"]
        D3["yield create_timer(72h)"]
        D4["yield call_activity(...)"]
    end

    subgraph FAST["âš¡ Fast Async Â· retry on failure"]
        direction TB
        F1["Inner workflow fan-out/fan-in"]
        F2["MCP tool calls via HTTP"]
        F3["LLM calls to Azure OpenAI"]
    end

    DURABLE ~~~ FAST

    style DURABLE fill:#d4edda,stroke:#28a745,stroke-width:2px
    style FAST fill:#fff3cd,stroke:#856404,stroke-width:2px
```

**Why not checkpoint every LLM call?** Adding DTS checkpoints to each LLM call would add ~200ms overhead per call and massively complicate the agent topology. The inner workflow runs in ~10â€“20 seconds â€” fast enough that retry-on-failure is the right strategy. If the worker crashes mid-inner-workflow, the entity call simply retries from the beginning.

### Pattern 2: Stateful Feedback Loop via Durable Entity

The `FraudAnalysisAgent` is registered as a DTS entity (`dafx-FraudAnalysisAgent`). The entity persists conversation history via `DurableAgentState`, enabling meaningful re-investigation:

```mermaid
sequenceDiagram
    participant O as DTS Orchestration
    participant E as Entity<br/>(dafx-FraudAnalysisAgent)
    participant S as DurableAgentState
    participant LLM as Azure OpenAI

    Note over O,S: First Investigation
    O->>E: yield fraud_agent.run(alert_json, session)
    E->>S: Load state (empty)
    E->>S: Append user message (alert)
    E->>LLM: Chat with full history
    LLM-->>E: Risk assessment
    E->>S: Append assistant response
    E->>S: Persist state â† saved to DTS storage
    E-->>O: AgentResponse (risk=0.85)

    Note over O,S: Analyst Rejects â€” Re-investigation
    O->>E: yield fraud_agent.run(feedback, same session!)
    E->>S: Load state (has alert + first analysis)
    E->>S: Append user message (feedback)
    E->>LLM: Chat with FULL history<br/>(alert + analysis + feedback)
    LLM-->>E: Deeper assessment
    E->>S: Append assistant response
    E->>S: Persist state
    E-->>O: AgentResponse (risk=0.72)
```

The agent doesn't start from scratch â€” it sees the original alert, its first analysis, AND the analyst's feedback. This is what makes re-investigation meaningful rather than redundant.

### Pattern 3: Ambient Detection Without LLM Overhead

Layer 1 uses fast Python rule evaluation, not LLM inference:

```python
# Rule: Multi-country login within 2 hours
if event.type == "login" and event.country != last_login_country:
    if time_delta < timedelta(hours=2):
        trigger_alert(event)  # â†’ Layer 2 DTS orchestration

# Rule: Spending spike > 3Ã— average
if event.type == "transaction" and event.amount > 3 * customer_average:
    trigger_alert(event)
```

At 1 event every 2â€“5 seconds, an LLM call (2â€“5s each) can't keep up. Rules handle the 95% of benign events at zero cost. The LLM's value is in Layer 2, where it reasons about complex multi-signal patterns.

---

## Durability Mechanics â€” Deep Dive

This section explains the **concrete mechanics** that make the architecture truly durable â€” not just the claim, but how it actually works under the hood.

### The Durability Stack

```mermaid
%%{init: {'theme': 'base'}}%%
flowchart TB
    subgraph YOUR_CODE["Your Code"]
        A["worker.py<br/>Orchestration generator + activities"]
    end

    subgraph AF["agent-framework DTS Layer"]
        B["DurableAIAgentWorker<br/>DurableAIAgentOrchestrationContext<br/>DurableAgentState"]
    end

    subgraph SDK["DTS Python SDK"]
        C["DurableTaskSchedulerWorker<br/>gRPC protocol"]
    end

    subgraph DTS["Azure Durable Task Scheduler"]
        D["Event Store Â· Timer Service<br/>Entity State Store Â· Task Queue"]
    end

    YOUR_CODE --> AF
    AF --> SDK
    SDK --> DTS

    style YOUR_CODE fill:#d4edda,stroke:#28a745,stroke-width:2px
    style AF fill:#e2d5f1,stroke:#6f42c1,stroke-width:2px
    style SDK fill:#cce5ff,stroke:#004085,stroke-width:2px
    style DTS fill:#fff3cd,stroke:#856404,stroke-width:2px
```

### 1. Event Sourcing â€” Checkpoints via `yield`

Every `yield` in the orchestration generator is a **checkpoint written to DTS's event store**:

```python
# Each yield writes an event to DTS's append-only log
response = yield fraud_agent.run(messages=alert, session=session)  # â† checkpoint
yield context.call_activity("notify_analyst", input=assessment)     # â† checkpoint
winner = yield when_any([wait_for_event(...), create_timer(72h)])    # â† checkpoint
```

These aren't in-memory variables â€” they're **persisted facts** in DTS storage. If the process crashes after step 2, DTS knows steps 1 and 2 completed because their completion events exist in the log.

### 2. Replay, Not Restore â€” How Crash Recovery Works

When a worker restarts after a crash, it doesn't "load state" â€” it **replays the orchestration function from the beginning**, but with a critical twist:

```mermaid
flowchart TB
    subgraph REPLAY["Orchestration Replay After Crash"]
        direction TB
        Y1["yield fraud_agent.run(...)"] -->|"DTS has completion event<br/>â†’ returns cached result instantly<br/>(no actual agent call)"| Y2["yield call_activity('notify_analyst')"]
        Y2 -->|"DTS has completion event<br/>â†’ returns cached result instantly"| Y3["yield when_any([event, timer])"]
        Y3 -->|"No completion event yet<br/>â†’ SUSPENDS HERE<br/>waiting for real event"| WAIT["â³ Waiting..."]
    end

    style REPLAY fill:#e8f4fd,stroke:#4A90D9,stroke-width:2px
    style WAIT fill:#ffeeba,stroke:#ffc107
```

The generator function re-executes, but each `yield` that already completed **returns its cached result instantly** without re-executing the actual work. The orchestration replays forward until it reaches the first incomplete step, then suspends. This is why your orchestration code must be **deterministic** â€” it's replayed, not restored.

### 3. External Events Survive Process Death

The `wait_for_external_event("AnalystDecision")` call is especially powerful â€” it creates a subscription that lives entirely in DTS storage:

```mermaid
sequenceDiagram
    participant W1 as Worker (original)
    participant DTS as DTS Storage
    participant W2 as Worker (restarted)
    participant BE as Backend
    participant UI as Analyst

    W1->>DTS: yield when_any([event, timer])
    Note over W1: Worker crashes! ğŸ’¥

    Note over DTS: Event subscription persists<br/>Timer ticks in DTS storage

    UI->>BE: POST /api/workflow/decision
    BE->>DTS: raise_orchestration_event<br/>("AnalystDecision", data)
    Note over DTS: Event stored in event log

    W2->>DTS: Start + long-poll for work
    DTS->>W2: Orchestration has pending event
    W2->>W2: Replay generator â†’ resume at yield
    W2->>W2: Process analyst decision
```

The event subscription, the timer countdown, and the eventual analyst response â€” **all live in DTS storage**, not in the worker's memory. The worker is just a stateless compute runtime that pulls work.

### 4. The Worker Is Stateless â€” All State Lives in DTS

This is the fundamental insight. The worker process holds **zero durable state**:

| What | Where It Lives | Survives Crash? |
|------|---------------|-----------------|
| Orchestration progress (which step) | DTS event log | âœ… Yes |
| Agent conversation history | DTS entity state (`DurableAgentState`) | âœ… Yes |
| Pending timers (72h analyst timeout) | DTS timer service | âœ… Yes |
| External event subscriptions | DTS event store | âœ… Yes |
| Activity results | DTS event log | âœ… Yes |
| In-flight LLM call | Worker memory | âŒ No (retried) |
| In-flight MCP tool call | Worker memory | âŒ No (retried) |

When the worker crashes, the only things lost are in-flight LLM/MCP calls â€” and those are inside the inner workflow, which retries as a unit when the entity operation is re-dispatched.

### 5. Entity State Persistence (`DurableAgentState`)

The agent-framework library stores the full agent conversation in a structured schema:

```mermaid
%%{init: {'theme': 'base'}}%%
erDiagram
    ENTITY["dafx-FraudAnalysisAgent"] {
        string instance_key "session_id"
    }

    STATE["DurableAgentState"] {
        string schema_version "v1.1.0"
    }

    ENTRY["ConversationEntry"] {
        string type "request or response"
        json messages "Message array"
        json tool_calls "ToolCall array"
        json errors "Error array"
        json token_usage "TokenUsage"
    }

    ENTITY ||--|| STATE : "persists"
    STATE ||--o{ ENTRY : "conversationHistory"
```

Every time the entity processes a request:
1. **Load** state from DTS â†’ `DurableAgentState`
2. **Append** user message (alert or analyst feedback)
3. **Rebuild** full chat history from all entries
4. **Call** LLM with complete conversation
5. **Append** assistant response
6. **Persist** state back to DTS via `self.persist_state()`

This is why the agent can re-investigate with full context â€” the conversation history is a **durable, append-only log** stored in DTS, not in worker memory.

---

## Running the Demo

This section covers how to run the reference implementation locally to see the patterns in action.

### Prerequisites

- **Docker** â€” for DTS emulator
- **Python 3.12+** with **uv**
- **Node.js 18+** â€” for React UI
- **Azure OpenAI** â€” with a deployed chat model
- **MCP Server** â€” Contoso tools on port 8000

### Service Startup Order

```mermaid
%%{init: {'theme': 'base'}}%%
flowchart LR
    S1["1ï¸âƒ£ DTS Emulator<br/>Port 8080"]
    S2["2ï¸âƒ£ MCP Server<br/>Port 8000"]
    S3["3ï¸âƒ£ Worker<br/>Pulls from DTS"]
    S4["4ï¸âƒ£ Backend<br/>Port 8001"]
    S5["5ï¸âƒ£ React UI<br/>Port 3000"]

    S1 --> S2 --> S3 --> S4 --> S5

    style S1 fill:#cce5ff,stroke:#004085
    style S2 fill:#d6d8db,stroke:#6c757d
    style S3 fill:#ffeeba,stroke:#ffc107
    style S4 fill:#d4edda,stroke:#28a745
    style S5 fill:#e8f4fd,stroke:#4A90D9
```

#### 1. Start DTS Emulator

```bash
docker run -d --name dts-emulator \
  -p 8080:8080 -p 8082:8082 \
  mcr.microsoft.com/dts/dts-emulator:latest
```

Dashboard: http://localhost:8082

> **Production:** Replace with Azure DTS â€” same SDK, just change `DTS_ENDPOINT`. See [provision_dts.ps1](provision_dts.ps1).

#### 2. Start MCP Server

```bash
cd mcp && uv run python mcp_service.py
```

#### 3. Start Worker

```bash
cd agentic_ai/workflow/fraud_detection_durable
uv sync && uv run python worker.py
```

#### 4. Start Backend

```bash
uv run python backend.py
```

#### 5. Start React UI

```bash
cd ui && npm install && npm run dev
# Open http://localhost:3000
```

---

## Demo Scenarios

### Scenario 1: Ambient Detection â†’ Auto-Clear

Watch the event feed â€” a multi-country login anomaly triggers automatic investigation. The agent assesses low risk â†’ auto-cleared without human involvement.

**What it proves:** Layer 1 rule engine triggering Layer 2 durable orchestration, with Layer 3 auto-clear path.

### Scenario 2: Ambient Detection â†’ HITL Approval

A spending spike triggers investigation. Agent assesses high risk â†’ analyst reviews in the UI â†’ approves "lock account" â†’ action executed.

**What it proves:** Full 3-layer flow including human-in-the-loop via durable external events.

### Scenario 3: Reject â†’ Stateful Re-investigation

Analyst rejects with feedback "check if VPN usage." Agent re-investigates with **full conversation history** (original alert + first analysis + analyst feedback) and produces a refined assessment.

**What it proves:** Entity state persistence enables meaningful multi-turn investigation across the HITL feedback loop.

### Scenario 4: Kill & Recover â€” The Durability Proof ğŸ’¥

This is the critical scenario â€” it proves the architecture delivers on its durability claims:

```mermaid
sequenceDiagram
    participant UI as Analyst
    participant BE as Backend
    participant DTS as DTS
    participant W1 as Worker v1

    UI->>BE: Start high-risk workflow
    BE->>DTS: schedule_new_orchestration()
    DTS->>W1: Work item: run orchestration
    W1->>W1: yield fraud_agent.run() âœ…
    W1->>W1: yield notify_analyst() âœ…
    W1->>DTS: yield wait_for_external_event()
    Note over W1: Status: Awaiting analyst review

    Note over W1: ğŸ’¥ taskkill /F /IM python.exe

    participant W2 as Worker v2
    Note over W2: uv run python worker.py

    UI->>BE: POST /api/workflow/decision (approve)
    BE->>DTS: raise_orchestration_event()
    DTS->>W2: Orchestration has pending event
    W2->>W2: Replay: yield agent.run() â†’ cached âœ…
    W2->>W2: Replay: yield notify() â†’ cached âœ…
    W2->>W2: Resume: when_any() â†’ event arrived!
    W2->>W2: yield execute_fraud_action() âœ…
    Note over W2: Workflow completes normally! ğŸ‰
```

**Steps:**
1. Start a high-risk workflow â†’ reaches "Awaiting analyst review"
2. `taskkill /F /IM python.exe` â€” kill all Python processes
3. `uv run python worker.py` â€” restart the worker
4. Submit analyst decision via UI
5. **Workflow completes normally** â€” DTS replayed the orchestration from its event log

---

## Project Structure

```
fraud_detection_durable/
â”œâ”€â”€ worker.py                       # DTS Worker: orchestration + agent entity + activities
â”œâ”€â”€ backend.py                      # FastAPI BFF: REST API, WebSocket, SSE, event producer
â”œâ”€â”€ event_producer.py               # Layer 1: telemetry generation + anomaly detection
â”œâ”€â”€ fraud_analysis_workflow.py      # Inner workflow: fan-out â†’ aggregate (Layer 2)
â”œâ”€â”€ provision_dts.ps1               # Azure DTS provisioning script
â”œâ”€â”€ .env                            # Configuration (Azure OpenAI, DTS, App Insights)
â”œâ”€â”€ pyproject.toml                  # Dependencies
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ PRODUCTION_ARCHITECTURE.md      # Production deployment on Azure Container Apps
â””â”€â”€ ui/                             # React/Vite UI
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ App.jsx                 # Main app: WebSocket + SSE connections
    â”‚   â””â”€â”€ components/
    â”‚       â”œâ”€â”€ ControlPanel.jsx    # Alert selector + start button
    â”‚       â”œâ”€â”€ WorkflowVisualizer.jsx  # React Flow DAG visualization
    â”‚       â”œâ”€â”€ AnalystDecisionPanel.jsx  # HITL approve/reject/feedback
    â”‚       â””â”€â”€ EventFeed.jsx       # Live telemetry feed (Layer 1)
    â””â”€â”€ package.json
```

## Configuration

| Variable | Description | Default |
|----------|-------------|---------|
| `AZURE_OPENAI_ENDPOINT` | Azure OpenAI endpoint | Required |
| `AZURE_OPENAI_CHAT_DEPLOYMENT` | Model deployment name | `gpt-4o` |
| `MCP_SERVER_URI` | MCP server URL | `http://localhost:8000/mcp` |
| `DTS_ENDPOINT` | DTS endpoint (local or Azure) | `http://localhost:8080` |
| `DTS_TASKHUB` | DTS task hub name | `default` |
| `ANALYST_APPROVAL_TIMEOUT_HOURS` | HITL timeout | `72` |
| `MAX_REVIEW_ATTEMPTS` | Max reject â†’ re-investigate cycles | `3` |
| `EVENT_PRODUCER_ENABLED` | Enable Layer 1 event producer | `true` |
| `EVENT_INTERVAL_SECONDS` | Seconds between telemetry events | `3` |
| `BACKEND_OBSERVABILITY` | Enable Application Insights | `false` |

## Production Deployment

For Azure Container Apps deployment topology, Managed Identity security, KEDA scaling, and cost estimation, see:

ğŸ‘‰ **[PRODUCTION_ARCHITECTURE.md](PRODUCTION_ARCHITECTURE.md)**

---

*Copyright (c) Microsoft. All rights reserved.*
