"""
Part 9 â€” Conversational Group Chat: User â†” Local Agent + LangGraph Agent

Demonstrates MAF's GroupChatBuilder orchestrating a multi-agent
discussion between:

  ğŸ‘” BusinessStrategist  â€” local MAF agent (Azure OpenAI)
  ğŸ—ï¸  TechnicalArchitect  â€” LangGraph agent served via MCP (port 8003)
  ğŸ“‹ Planner             â€” local MAF agent that synthesizes the plan
  ğŸ¯ Facilitator         â€” LLM orchestrator that decides who speaks next

The Facilitator routes the conversation: experts discuss first, then
the Planner delivers a consolidated plan inline â€” no separate
synthesis step needed.  This is more efficient than post-hoc synthesis.

Simulates a multi-turn conversation with predefined questions:
  1. User poses an initial topic â†’ experts discuss â†’ Planner delivers plan
  2. User asks a follow-up â†’ experts discuss deeper â†’ Planner updates plan
  3. The TechnicalArchitect (LangGraph) remembers prior turns
     via MemorySaver â€” the proxy sends only the NEW message

This showcases stateful cross-framework orchestration:
  â€¢ MCP session provides a persistent connection
  â€¢ LangGraph MemorySaver keeps conversation history server-side
  â€¢ MCPProxyAgent sends only the latest message (not full history)
  â€¢ GroupChatBuilder treats both local and remote agents identically
  â€¢ Planner delivers the plan as a participant (no extra LLM call)

Prerequisites:
    mcp_server_langgraph.py must be running on http://localhost:8003/mcp

Usage:
    cd agentic_ai/agents/mcp_agent_demo
    uv run python workflow_group_chat.py
"""

import asyncio
import os
import sys
import uuid
from typing import Any, cast

from dotenv import load_dotenv

# Load credentials from the shared mcp/.env
env_path = os.path.join(os.path.dirname(__file__), "..", "..", "..", "mcp", ".env")
load_dotenv(env_path)

from agent_framework import (
    AgentResponse,
    AgentResponseUpdate,
    BaseAgent,
    Content,
    MCPStreamableHTTPTool,
    Message,
)
from agent_framework.azure import AzureOpenAIChatClient
from agent_framework.orchestrations import GroupChatBuilder


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  MCPProxyAgent â€” BaseAgent that bridges to a remote MCP tool
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class MCPProxyAgent(BaseAgent):
    """BaseAgent that forwards the latest message to a remote MCP tool.

    Because the MCP server is **stateful** (maintains conversation history
    per session via MemorySaver), we only need to send the LATEST message
    â€” not the full conversation history.  The server accumulates context
    automatically.
    """

    def __init__(
        self,
        *,
        mcp_tool: MCPStreamableHTTPTool,
        tool_name: str,
        name: str = "mcp_proxy",
        description: str | None = None,
        param_name: str = "question",
        **kwargs: Any,
    ) -> None:
        super().__init__(name=name, description=description, **kwargs)
        self._mcp_tool = mcp_tool
        self._tool_name = tool_name
        self._param_name = param_name

    # Must be a regular def â€” GroupChatBuilder iterates with
    # ``async for update in agent.run(stream=True)``
    def run(self, messages: Any = None, *, stream: bool = False, **kwargs: Any) -> Any:
        if stream:
            return self._run_stream(messages)
        return self._run_impl(messages)

    async def _run_impl(self, messages: Any) -> AgentResponse:
        result_text = await self._call(messages)
        return AgentResponse(
            messages=[Message("assistant", [result_text], author_name=self.name)],
            response_id=f"proxy-{uuid.uuid4().hex[:8]}",
            agent_id=self.id,
        )

    async def _run_stream(self, messages: Any):
        result_text = await self._call(messages)
        yield AgentResponseUpdate(
            contents=[Content.from_text(result_text)],
            role="assistant",
            author_name=self.name,
            agent_id=self.id,
            response_id=f"proxy-{uuid.uuid4().hex[:8]}",
        )

    async def _call(self, messages: Any) -> str:
        """Extract only the LATEST message and forward to the stateful MCP tool.

        The server-side LangGraph agent (with MemorySaver) already has the
        full conversation history for this session, so we only send the
        new content.  This is the key difference from the old implementation
        which manually forwarded the entire conversation each turn.
        """
        latest_text = _extract_latest_message(messages)
        result = await self._mcp_tool.call_tool(
            self._tool_name, **{self._param_name: latest_text}
        )
        if isinstance(result, (list, tuple)):
            return "\n".join(
                c.text for c in result if hasattr(c, "text") and c.text
            ) or str(result)
        return result.text if hasattr(result, "text") and result.text else str(result)


def _extract_latest_message(messages: Any) -> str:
    """Extract just the last meaningful message from the conversation.

    The orchestrator passes the full conversation as list[Message].
    We only need the LAST message since the MCP server is stateful
    and already has prior turns in its MemorySaver checkpointer.
    """
    if isinstance(messages, str):
        return messages
    if isinstance(messages, Message):
        return messages.text or ""
    if isinstance(messages, (list, tuple)):
        # Walk backwards to find the last message with content
        for m in reversed(messages):
            if isinstance(m, Message):
                text = m.text or ""
                if text.strip():
                    name = m.author_name or m.role or "someone"
                    return f"[{name}]: {text}"
            elif isinstance(m, str) and m.strip():
                return m
    return str(messages) if messages else ""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  MAIN â€” Conversational Group Chat with follow-ups
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


async def _run_group_chat(
    topic: str,
    strategist: Any,
    architect: MCPProxyAgent,
    planner: Any,
    facilitator: Any,
    turn_label: str = "Initial Discussion",
) -> int:
    """Run one round of group chat.

    Returns:
        round_count â€” number of visible participant messages.
    """
    workflow = GroupChatBuilder(
        participants=[strategist, architect, planner],
        orchestrator_agent=facilitator,
        max_rounds=6,
        intermediate_outputs=True,
    ).build()

    print()
    print(f"â”{'â”' * 76}â”")
    print(f"  ğŸ“‹ {turn_label}")
    print(f"â”{'â”' * 76}â”")
    print(f"  {topic[:120]}{'â€¦' if len(topic) > 120 else ''}")
    print()

    round_num = 0
    async for event in workflow.run(topic, stream=True):
        if event.type == "output":
            data = event.data
            if not isinstance(data, list):
                continue
            for msg in cast(list[Message], data):
                name = msg.author_name or ""
                text = msg.text or ""
                if not text.strip():
                    continue

                # Skip the initial user message echo
                if not name or msg.role == "user":
                    continue

                if name == "BusinessStrategist":
                    icon, framework = "ğŸ‘”", "MAF Agent"
                elif name == "TechnicalArchitect":
                    icon, framework = "ğŸ—ï¸", "LangGraph via MCP"
                elif name == "Planner":
                    icon, framework = "ğŸ“‹", "MAF Agent"
                else:
                    icon, framework = "ğŸ’¬", name

                round_num += 1
                print(f"{'â•' * 78}")
                print(f"  Round {round_num} â€” {icon} {name}  [{framework}]")
                print(f"{'â•' * 78}")
                print()
                print(text)
                print()

    return round_num





# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Predefined conversation â€” simulates a multi-turn user interaction
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CONVERSATION = [
    {
        "role": "user",
        "label": "Initial Topic",
        "message": (
            "Our company is a mid-size e-commerce retailer with a legacy "
            "on-premise monolithic Java application serving 2M monthly users. "
            "We want to migrate to a cloud-native architecture to improve "
            "scalability, reduce operational costs, and enable AI-powered "
            "personalization. We have a $500K budget and 12-month timeline. "
            "Discuss the strategy, architecture, and implementation approach."
        ),
    },
    {
        "role": "user",
        "label": "Follow-up: Risk & Phasing",
        "message": (
            "Thanks for the plan. I have concerns about risk. "
            "What are the biggest risks with this migration, and how "
            "should we phase the rollout to minimize disruption to our "
            "existing customers? Can we keep the monolith running in "
            "parallel during the transition?"
        ),
    },
]


async def main() -> None:
    endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT")
    api_key = os.environ.get("AZURE_OPENAI_API_KEY")
    deployment = os.environ.get("AZURE_OPENAI_CHAT_DEPLOYMENT", "gpt-4.1")
    api_version = os.environ.get("AZURE_OPENAI_API_VERSION", "2025-03-01-preview")

    if not endpoint or not api_key:
        print("ERROR: Set AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY in mcp/.env")
        sys.exit(1)

    mcp_server_url = "http://localhost:8003/mcp"

    print("=" * 78)
    print("ğŸ—£ï¸  Conversational Group Chat â€” User â†” MAF + LangGraph Agents")
    print("=" * 78)
    print()
    print("  Simulating a multi-turn conversation with predefined questions.")
    print("  The TechnicalArchitect (LangGraph) remembers prior turns via")
    print("  MemorySaver â€” the proxy sends only the NEW message each turn.")
    print()

    # â”€â”€ Connect to the stateful LangGraph MCP server â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    async with MCPStreamableHTTPTool(
        name="technical_architect",
        description="Remote Technical Architect (LangGraph) via MCP",
        url=mcp_server_url,
    ) as mcp_tool:
        print(f"ğŸ”— Connected to LangGraph MCP server at {mcp_server_url}")
        print(f"   Available tools: {[t.name for t in mcp_tool.functions]}")
        print(f"   Session: stateful (server maintains conversation history)")
        print()

        client = AzureOpenAIChatClient(
            api_key=api_key,
            endpoint=endpoint,
            deployment_name=deployment,
            api_version=api_version,
        )

        # â”€â”€ Participant 1: Local Business Strategist (MAF Agent) â”€â”€â”€â”€â”€â”€â”€â”€â”€
        strategist = client.as_agent(
            name="BusinessStrategist",
            instructions=(
                "You are a Business Strategist specializing in digital "
                "transformation and cloud migration. Focus on:\n"
                "â€¢ Business impact and ROI analysis\n"
                "â€¢ Customer experience implications\n"
                "â€¢ Risk mitigation and change management\n"
                "â€¢ Competitive advantage and market positioning\n"
                "â€¢ Timeline and budget considerations\n\n"
                "Build on what others have said in the conversation. "
                "Be concise but insightful â€” 2-3 paragraphs max."
            ),
        )

        # â”€â”€ Participant 2: Remote Technical Architect (LangGraph via MCP) â”€
        architect = MCPProxyAgent(
            mcp_tool=mcp_tool,
            tool_name="ask_architect",
            param_name="question",
            name="TechnicalArchitect",
            description=(
                "Technical Architect providing architecture design, "
                "technology stack recommendations, cloud infrastructure "
                "planning, and migration strategies. Built with LangGraph, "
                "served via MCP."
            ),
        )

        # â”€â”€ Participant 3: Planner (synthesizes discussion into plan) â”€â”€â”€â”€
        planner = client.as_agent(
            name="Planner",
            instructions=(
                "You are a senior project planner participating in a "
                "strategy meeting alongside a BusinessStrategist and "
                "TechnicalArchitect.\n\n"
                "When the facilitator calls on you, synthesize the team's "
                "discussion into a structured, actionable plan:\n"
                "  â€¢ Numbered phases with clear deliverables\n"
                "  â€¢ Timeline and budget allocation\n"
                "  â€¢ Key milestones and success metrics\n"
                "  â€¢ Top risks with mitigations\n"
                "  â€¢ Concrete next steps\n\n"
                "Incorporate BOTH business and technical recommendations "
                "from your teammates. Write it as a professional plan "
                "the client can act on. Be comprehensive but concise."
            ),
        )

        # â”€â”€ Orchestrator: Lightweight Facilitator â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        facilitator = client.as_agent(
            name="Facilitator",
            instructions=(
                "You are a discussion facilitator for a strategy meeting. "
                "Three experts are available:\n\n"
                "  â€¢ BusinessStrategist â€” business impact, ROI, risk, "
                "go-to-market\n"
                "  â€¢ TechnicalArchitect â€” architecture, tech stack, "
                "migration patterns, infrastructure\n"
                "  â€¢ Planner â€” synthesizes discussion into an actionable "
                "plan\n\n"
                "Your workflow:\n"
                "  1. Start with BusinessStrategist for business perspective\n"
                "  2. Then TechnicalArchitect for technical depth\n"
                "  3. Alternate if needed for a richer discussion\n"
                "  4. When both experts have contributed enough, call on "
                "Planner to deliver the consolidated plan\n"
                "  5. After Planner delivers the plan, TERMINATE\n\n"
                "Do NOT let Planner speak until the experts have "
                "had a meaningful exchange."
            ),
        )

        # â”€â”€ Print participant info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print("â”" * 78)
        print("ğŸ‘¥ PARTICIPANTS")
        print("â”" * 78)
        print("   ğŸ‘” BusinessStrategist â€” local MAF agent (Azure OpenAI)")
        print("   ğŸ—ï¸  TechnicalArchitect â€” LangGraph agent via MCP (port 8003)")
        print("   ğŸ“‹ Planner           â€” local MAF agent (synthesizes plan)")
        print("   ğŸ¯ Facilitator       â€” LLM orchestrator (decides who speaks)")
        print("â”" * 78)
        print()

        # â”€â”€ Run the predefined conversation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        total_rounds = 0

        for turn_num, turn in enumerate(CONVERSATION, 1):
            is_last_turn = turn_num == len(CONVERSATION)

            # Print what the "user" is saying
            print()
            print(f"{'â–“' * 78}")
            print(f"  ğŸ‘¤ USER (Turn {turn_num})")
            print(f"{'â–“' * 78}")
            print(f"  {turn['message']}")
            print()

            rounds = await _run_group_chat(
                turn["message"], strategist, architect, planner,
                facilitator,
                turn_label=f"Discussion #{turn_num}: {turn['label']}",
            )
            total_rounds += rounds

            print("â”" * 78)
            print(f"âœ…  Discussion #{turn_num} complete â€” {rounds} rounds.")
            print("â”" * 78)

            if not is_last_turn:
                print()
                print("â”" * 78)
                print("  ğŸ’¡ The TechnicalArchitect remembers the prior discussion")
                print("     (stateful MCP session with LangGraph MemorySaver)")
                print("     Only the NEW question is sent â€” server keeps history.")
                print()
                print("  ğŸ“ The user reviews the plan and asks a follow-up...")
                print("â”" * 78)

        # â”€â”€ Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print()
        print("=" * 78)
        print(f"ğŸ“Š  Session Summary: {len(CONVERSATION)} discussions, {total_rounds} total rounds")
        print("=" * 78)
        print("   Key points demonstrated:")
        print("     â€¢ Multi-turn conversation with predefined questions")
        print("     â€¢ Planner delivers plan inline â€” no separate synthesis step")
        print("     â€¢ Facilitator orchestrates: experts discuss â†’ Planner delivers")
        print("     â€¢ LangGraph agent remembers prior context (MemorySaver)")
        print("     â€¢ MCP session provides persistent stateful connection")
        print("     â€¢ Proxy sends only latest message (server keeps history)")
        print("     â€¢ Cross-framework: MAF orchestrator + LangGraph agent via MCP")
        print("=" * 78)


if __name__ == "__main__":
    asyncio.run(main())
